{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and labeled frames from C:/Users/HP/Desktop/Road_quality/1.mp4\n",
      "Extracted and labeled frames from C:/Users/HP/Desktop/Road_quality/2.mp4\n",
      "Extracted and labeled frames from C:/Users/HP/Desktop/Road_quality/3.mp4\n",
      "Total extracted and labeled frames: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\HP/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:48<00:00, 962kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9763183891773224\n",
      "Epoch 2/10, Loss: 0.7132951269547144\n",
      "Epoch 3/10, Loss: 0.7123153110345205\n",
      "Epoch 4/10, Loss: 0.7122755944728851\n",
      "Epoch 5/10, Loss: 0.7251083329319954\n",
      "Epoch 6/10, Loss: 0.706922655304273\n",
      "Epoch 7/10, Loss: 0.705375832815965\n",
      "Epoch 8/10, Loss: 0.7073759237925211\n",
      "Epoch 9/10, Loss: 0.711012452840805\n",
      "Epoch 10/10, Loss: 0.6981175070007642\n",
      "Epoch 1/10, Loss: 0.8753547022740046\n",
      "Epoch 2/10, Loss: 0.7569331054886183\n",
      "Epoch 3/10, Loss: 0.7108229746421179\n",
      "Epoch 4/10, Loss: 0.7065151905020078\n",
      "Epoch 5/10, Loss: 0.6964832295974096\n",
      "Epoch 6/10, Loss: 0.7030665750304858\n",
      "Epoch 7/10, Loss: 0.7002002273996671\n",
      "Epoch 8/10, Loss: 0.7214961846669515\n",
      "Epoch 9/10, Loss: 0.7214476491014162\n",
      "Epoch 10/10, Loss: 0.7212604582309723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7100\\3436933895.py:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  road_type_model.load_state_dict(torch.load('road_type_model.pth', map_location=device))\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7100\\3436933895.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  road_quality_model.load_state_dict(torch.load('road_quality_model.pth', map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract Frames from Multiple Videos and Auto-label as 'asphalt' and 'good'\n",
    "videos = [\n",
    "    \"C:/Users/HP/Desktop/Road_quality/1.mp4\",\n",
    "    \"C:/Users/HP/Desktop/Road_quality/2.mp4\",\n",
    "    \"C:/Users/HP/Desktop/Road_quality/3.mp4\"\n",
    "]\n",
    "\n",
    "output_dir_asphalt = 'frames/train/asphalt'\n",
    "output_dir_good = 'frames/train/good'\n",
    "os.makedirs(output_dir_asphalt, exist_ok=True)\n",
    "os.makedirs(output_dir_good, exist_ok=True)\n",
    "\n",
    "def extract_frames_and_label(videos, output_dir, frame_skip=30):\n",
    "    frame_count = 0\n",
    "    for video_path in videos:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        count = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if count % frame_skip == 0:\n",
    "                frame_path_asphalt = os.path.join(output_dir_asphalt, f'frame_{frame_count}.jpg')\n",
    "                frame_path_good = os.path.join(output_dir_good, f'frame_{frame_count}.jpg')\n",
    "                cv2.imwrite(frame_path_asphalt, frame)\n",
    "                cv2.imwrite(frame_path_good, frame)\n",
    "                frame_count += 1\n",
    "            count += 1\n",
    "        cap.release()\n",
    "        print(f\"Extracted and labeled frames from {video_path}\")\n",
    "    print(f\"Total extracted and labeled frames: {frame_count}\")\n",
    "\n",
    "extract_frames_and_label(videos, output_dir_asphalt)\n",
    "\n",
    "# Step 2: Train the Model Using Extracted Frames\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('frames/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "    return model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Train road type model\n",
    "road_type_model = get_model(num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(road_type_model.parameters(), lr=0.001)\n",
    "road_type_model = train_model(road_type_model, criterion, optimizer, num_epochs=10)\n",
    "torch.save(road_type_model.state_dict(), 'road_type_model.pth')\n",
    "\n",
    "# Train road quality model\n",
    "road_quality_model = get_model(num_classes=2)\n",
    "optimizer = optim.Adam(road_quality_model.parameters(), lr=0.001)\n",
    "road_quality_model = train_model(road_quality_model, criterion, optimizer, num_epochs=10)\n",
    "torch.save(road_quality_model.state_dict(), 'road_quality_model.pth')\n",
    "\n",
    "# Step 3: Real-Time Video Classification\n",
    "road_type_model.load_state_dict(torch.load('road_type_model.pth', map_location=device))\n",
    "road_type_model.eval()\n",
    "\n",
    "road_quality_model.load_state_dict(torch.load('road_quality_model.pth', map_location=device))\n",
    "road_quality_model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def classify_frame(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        road_type_pred = road_type_model(input_tensor)\n",
    "        road_type_label = 'Asphalt' if torch.argmax(road_type_pred) == 0 else 'Concrete'\n",
    "\n",
    "        road_quality_pred = road_quality_model(input_tensor)\n",
    "        road_quality_label = 'Good' if torch.argmax(road_quality_pred) == 0 else 'Bad (Pothole)'\n",
    "\n",
    "    return road_type_label, road_quality_label\n",
    "\n",
    "video_capture = cv2.VideoCapture('C:/Users/HP/Desktop/Road_quality/1.mp4')  # Change to a video file path if needed\n",
    "\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    road_type, road_quality = classify_frame(frame)\n",
    "    cv2.putText(frame, f'Road Type: {road_type}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Road Quality: {road_quality}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Road Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'asphalt' has been split into val and test sets.\n",
      "Category 'good' has been split into val and test sets.\n",
      "Frame splitting into val and test sets completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Path to your `train` directory\n",
    "source_path = 'frames/train'\n",
    "categories = ['asphalt', 'good']  # Adjust these as necessary based on your labels\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Create val and test directories\n",
    "for category in categories:\n",
    "    os.makedirs(f'frames/val/{category}', exist_ok=True)\n",
    "    os.makedirs(f'frames/test/{category}', exist_ok=True)\n",
    "\n",
    "    # Path to the category directory in the `train` folder\n",
    "    category_path = os.path.join(source_path, category)\n",
    "    if not os.path.exists(category_path):\n",
    "        print(f\"Category directory {category_path} does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Get all files in the category directory\n",
    "    files = os.listdir(category_path)\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # Calculate the split indices\n",
    "    train_split = int(len(files) * train_ratio)\n",
    "    val_split = int(len(files) * (train_ratio + val_ratio))\n",
    "\n",
    "    # Split the files into training, validation, and testing sets\n",
    "    val_files = files[train_split:val_split]\n",
    "    test_files = files[val_split:]\n",
    "\n",
    "    # Move the files to the respective directories\n",
    "    for file in val_files:\n",
    "        shutil.move(os.path.join(category_path, file), f'frames/val/{category}/{file}')\n",
    "    for file in test_files:\n",
    "        shutil.move(os.path.join(category_path, file), f'frames/test/{category}/{file}')\n",
    "\n",
    "    print(f\"Category '{category}' has been split into val and test sets.\")\n",
    "\n",
    "print(\"Frame splitting into val and test sets completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Real-Time Video Classification\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the state dictionaries for the trained models with weights_only=True\n",
    "road_type_model.load_state_dict(torch.load('road_type_model.pth', map_location=device, weights_only=True))\n",
    "road_type_model.eval()\n",
    "\n",
    "road_quality_model.load_state_dict(torch.load('road_quality_model.pth', map_location=device, weights_only=True))\n",
    "road_quality_model.eval()\n",
    "\n",
    "# Define transformation for input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to classify a frame\n",
    "def classify_frame(frame):\n",
    "    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        road_type_pred = road_type_model(input_tensor)\n",
    "        road_type_label = 'Asphalt' if torch.argmax(road_type_pred) == 0 else 'Concrete'\n",
    "\n",
    "        road_quality_pred = road_quality_model(input_tensor)\n",
    "        road_quality_label = 'Good' if torch.argmax(road_quality_pred) == 0 else 'Bad (Pothole)'\n",
    "\n",
    "    return road_type_label, road_quality_label\n",
    "\n",
    "# Run real-time classification on a custom video file\n",
    "video_capture = cv2.VideoCapture('C:/Users/HP/Desktop/Road_quality/1.mp4')  # Replace with your custom video path\n",
    "\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    road_type, road_quality = classify_frame(frame)\n",
    "    cv2.putText(frame, f'Road Type: {road_type}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f'Road Quality: {road_quality}', (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Road Classification', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
